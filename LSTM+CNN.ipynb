{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jjuhyeok/Anomaly_Detection/blob/David/LSTM%2BCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2kzB8-T_aWv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "import time\n",
        "import math\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from easydict import EasyDict as edict\n",
        "from torch.optim.optimizer import Optimizer\n",
        "\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, QuantileTransformer, KBinsDiscretizer\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Mount Drive**"
      ],
      "metadata": {
        "id": "fqQiDsDQLSd4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_NUXcpk_jNU"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive')\n",
        "root = '/content/drive/My Drive/smartfactory'\n",
        "# os.chdir(root)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Helper Functions**"
      ],
      "metadata": {
        "id": "argpHBCCLhvn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_engineering(df):\n",
        "  \"\"\"\n",
        "      Feature engineering using actual physical laws\n",
        "  \"\"\"\n",
        "  df[\"heat_rate\"] = df['air_inflow'] * (df['air_end_temp'] - 25)\n",
        "  df[\"power_consumption\"] = df[\"motor_current\"] * df[\"motor_vibe\"]\n",
        "  df[\"power_output\"] = df[\"motor_current\"] * df[\"motor_rpm\"]\n",
        "  df[\"compressor_efficiency\"] = df[\"motor_vibe\"] / df[\"motor_rpm\"]\n",
        "  df[\"compressor_temp_change\"] = df[\"air_end_temp\"] - 25\n",
        "  df[\"compressor_heat_reject\"] = 1.293 * df[\"compressor_temp_change\"] * df[\"air_inflow\"]\n",
        "  df[\"air_mass_flow\"] = 1.293 * df[\"air_inflow\"]\n",
        "  df['air_velocity'] = df[\"air_inflow\"] / (3.14 * 0.05 * 0.05)\n",
        "  df['air_pressure'] = 101.325 + 0.5 * 1.293 * (df[\"air_velocity\"]**2)\n",
        "  df[\"air_enthalpy\"] = 700 * df[\"air_inflow\"] * df[\"air_end_temp\"]\n",
        "  df[\"compression_ratio\"] = df[\"out_pressure\"] / 101.325\n",
        "  df[\"temp_pressure_ration\"] = (25 / df[\"air_end_temp\"]) * (101.325 / df[\"out_pressure\"])\n",
        "  return df\n",
        "\n",
        "\n",
        "def to_numeric(df):\n",
        "  \"\"\"\n",
        "      Change numerics to numeric type\n",
        "  \"\"\"\n",
        "  \n",
        "  for col in list(df.columns):\n",
        "      df[col] = df[col].apply(pd.to_numeric, errors='ignore')\n",
        "  return df\n",
        "\n",
        "\n",
        "def get_columns(df):\n",
        "  return list(df.columns)\n",
        "\n",
        "\n",
        "def csv_download(df, filename):\n",
        "  df.to_csv(f'{filename}.csv', index=False)\n",
        "  files.download(f'{filename}.csv')\n",
        "\n",
        "\n",
        "def filter_data(df, idx):\n",
        "  df1 = pd.DataFrame()\n",
        "  for id in idx:\n",
        "    df1 = pd.concat([df1, df[df[\"type\"] == id]], axis=0)\n",
        "  return df1\n"
      ],
      "metadata": {
        "id": "cFSg8hGGr8rR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Preprocessing**"
      ],
      "metadata": {
        "id": "tEyWLMUKrdxl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Load and Feature Engineer Data**"
      ],
      "metadata": {
        "id": "SZYF_OPYqwAE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv(os.path.join(root, \"train_data.csv\"))\n",
        "test = pd.read_csv(os.path.join(root, \"test_data.csv\"))"
      ],
      "metadata": {
        "id": "O-DeWeQ-sDXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = feature_engineering(train)\n",
        "test = feature_engineering(test)\n",
        "\n",
        "columns = get_columns(train)\n",
        "columns = [x for x in columns if x != \"type\"]"
      ],
      "metadata": {
        "id": "McMerZB7sWmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Scaling**"
      ],
      "metadata": {
        "id": "Ocia7TVbq3ra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_on_idx = [4]      # Select the class types you want to train on\n",
        "df_train = filter_data(train, train_on_idx)"
      ],
      "metadata": {
        "id": "P7or504-s5tC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WfEsV1Aum5h7"
      },
      "outputs": [],
      "source": [
        "# scaler = MinMaxScaler(clip=True)\n",
        "scaler = StandardScaler()\n",
        "train_data = scaler.fit_transform(df_train[columns])\n",
        "train_data = pd.DataFrame(data=train_data, index=df_train.index.tolist(), columns=columns)\n",
        "train_data = pd.concat([train_data, df_train[['type']]], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5513_K3mvQzc"
      },
      "outputs": [],
      "source": [
        "train_inv_transform = np.vstack((scaler.mean_, scaler.scale_))\n",
        "train_inv_transform = pd.DataFrame(train_inv_transform, columns=columns, index=[\"mean\", \"stdv\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Checkpoint**"
      ],
      "metadata": {
        "id": "5KkDUVWbq6rJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0p-s5g8SKnnU"
      },
      "outputs": [],
      "source": [
        "trial = 0\n",
        "ckpt_dir = Path(f'{root}/logs_competition/ckpt_{trial}')\n",
        "if ckpt_dir.exists():\n",
        "    shutil.rmtree(ckpt_dir)\n",
        "\n",
        "ckpt_dir.mkdir(parents=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NzJur4pUBbjM"
      },
      "outputs": [],
      "source": [
        "pickle.dump(train_inv_transform, open(f'{ckpt_dir}/train_ss.pkl','wb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Train-Val Split**"
      ],
      "metadata": {
        "id": "N4R7K5DSrJ16"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKOkE3Do2SFn"
      },
      "outputs": [],
      "source": [
        "def train_val_split(df, split, start=None):\n",
        "  \"\"\"\"\n",
        "      Randomly take a portion of data\n",
        "  \"\"\"\n",
        "  val = int(len(df) * (1 - split))\n",
        "  if val != 0:\n",
        "    index = random.randint(0, len(df) - val)\n",
        "\n",
        "    train = pd.concat([df.iloc[0 : index, :], df.iloc[index + val: , :]], axis=0)\n",
        "    valid = df.iloc[index: index + val, :]\n",
        "    if start:\n",
        "      index = start\n",
        "    else:\n",
        "      index = valid.index.tolist()[0]\n",
        "\n",
        "    return train, valid, index\n",
        "  \n",
        "  return df, pd.DataFrame(columns=list(df.columns)), None\n",
        "\n",
        "\n",
        "def class_based_split(df, split, idx, start):\n",
        "  \"\"\"\n",
        "      Keep the split proportion within each class\n",
        "  \"\"\"\n",
        "  train = pd.DataFrame()\n",
        "  val = pd.DataFrame()\n",
        "  start_ = []\n",
        "  for i, id in enumerate(idx):\n",
        "    type_i = df[df['type'] == id]\n",
        "    type_i_train, type_i_val, type_i_index = train_val_split(type_i, split, start[i])\n",
        "    start_.append(type_i_index)\n",
        "    train = pd.concat([train, type_i_train], axis=0, ignore_index=True)\n",
        "    val = pd.concat([val, type_i_val], axis=0, ignore_index=True)\n",
        "  return train, val, start_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbqCsZuhB93F"
      },
      "outputs": [],
      "source": [
        "start = [None]*len(train_on_idx)\n",
        "training_data, validation_data, start = class_based_split(train_data, split=0.8, idx=train_on_idx, start=start)\n",
        "print(f\"Validation start index for each type {train_on_idx} is {start}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Construct Dataset**"
      ],
      "metadata": {
        "id": "d7izQHszrOq6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIyQINCPGo0A"
      },
      "outputs": [],
      "source": [
        "def get_prev_data(df, index, backward, features, inclusive=True):\n",
        "    start = index - backward\n",
        "    end = index\n",
        "    if start < 0:\n",
        "        df_temp = df.loc[0 : end, features]\n",
        "        df_temp = pd.concat([pd.DataFrame.from_records([[0.]*len(features)]*np.abs(start), columns=features), df_temp], axis=0)\n",
        "        df_temp = df_temp.replace(to_replace=0.0, method='bfill')\n",
        "\n",
        "    else:\n",
        "        df_temp = df.loc[start : end, features]\n",
        "\n",
        "    if not inclusive:\n",
        "        df_temp = df_temp.iloc[:-1]\n",
        "\n",
        "    df_temp.reset_index(drop=True, inplace=True)\n",
        "    return df_temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XVJZmgeKFLLS"
      },
      "outputs": [],
      "source": [
        "class MyDataset(Dataset):\n",
        "  def __init__(self, data, features, added_features, look_forward, look_backward, duplicate=0):\n",
        "    \"\"\"\n",
        "        data: Input data\n",
        "        features: Features reconstruction is done on\n",
        "        added_features: Feature engineered or other features\n",
        "        look_forward: a list of numbers that will be randomly selected. Determines our time window to the future\n",
        "        look_backwaed: a list of numbers that will be randomly selected. Determines our time window to the past\n",
        "        duplicat: how many times do you want to duplicate each sequence\n",
        "    \"\"\"\n",
        "    self.input = []\n",
        "    self.prev = []\n",
        "    self.additional = []\n",
        "    size = len(data)\n",
        "\n",
        "    for i in tqdm(range(size)):\n",
        "      forward = random.choice(look_forward)\n",
        "      backward = random.choice(look_backward)\n",
        "      \n",
        "      prev_data = get_prev_data(data, i, backward, features + added_features, inclusive=True)\n",
        "      input_data = data.loc[[i], features]\n",
        "      additional_data = data.loc[[i], added_features]\n",
        "\n",
        "      if duplicate:\n",
        "        for i in range(duplicate):\n",
        "          self.prev.append(prev_data.values.tolist())\n",
        "          self.input.append(input_data.iloc[0, :].values.tolist())\n",
        "          self.additional.append(additional_data.iloc[0, :].values.tolist())\n",
        "\n",
        "      self.prev.append(prev_data.values.tolist())\n",
        "      self.input.append(input_data.iloc[0, :].values.tolist())\n",
        "      self.additional.append(additional_data.iloc[0, :].values.tolist())\n",
        "\n",
        "    self.input = torch.FloatTensor(self.input)            \n",
        "    self.prev = torch.FloatTensor(self.prev)        \n",
        "    self.additional = torch.FloatTensor(self.additional)\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    return {\"input\": self.input[index],\n",
        "            \"prev\": self.prev[index],\n",
        "            \"additional\": self.additional[index]\n",
        "    }\n",
        "  def __len__(self):\n",
        "    return len(self.input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hcucArtjBiD"
      },
      "outputs": [],
      "source": [
        "features = ['air_inflow', 'air_end_temp', 'out_pressure', 'motor_current', 'motor_rpm', 'motor_temp', 'motor_vibe']\n",
        "added_features = ['heat_rate', 'power_consumption', 'power_output', 'compressor_efficiency', 'compressor_temp_change', 'compressor_heat_reject', 'air_mass_flow', 'air_velocity', 'air_pressure', 'air_enthalpy', 'compression_ratio', 'temp_pressure_ration']\n",
        "\n",
        "train_dataset = MyDataset(data=training_data, features=features, added_features=added_features, look_forward=[0], look_backward=[2], duplicate=10)\n",
        "val_dataset = MyDataset(data=validation_data, features=features, added_features=added_features, look_forward=[0], look_backward=[2], duplicate=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hyrx6GZ0DrQu"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Training**"
      ],
      "metadata": {
        "id": "mWW3mFcmrVZn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TeHV4RPnnonK"
      },
      "outputs": [],
      "source": [
        "# code inspired from: https://github.com/anandsaha/pytorch.cyclic.learning.rate/blob/master/cls.py\n",
        "class CyclicLR(object):\n",
        "    def __init__(self, optimizer, base_lr=1e-3, max_lr=6e-3,\n",
        "                 step_size=2000, mode='triangular', gamma=1.,\n",
        "                 scale_fn=None, scale_mode='cycle', last_batch_iteration=-1):\n",
        "\n",
        "        if not isinstance(optimizer, Optimizer):\n",
        "            raise TypeError('{} is not an Optimizer'.format(\n",
        "                type(optimizer).__name__))\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "        if isinstance(base_lr, list) or isinstance(base_lr, tuple):\n",
        "            if len(base_lr) != len(optimizer.param_groups):\n",
        "                raise ValueError(\"expected {} base_lr, got {}\".format(\n",
        "                    len(optimizer.param_groups), len(base_lr)))\n",
        "            self.base_lrs = list(base_lr)\n",
        "        else:\n",
        "            self.base_lrs = [base_lr] * len(optimizer.param_groups)\n",
        "\n",
        "        if isinstance(max_lr, list) or isinstance(max_lr, tuple):\n",
        "            if len(max_lr) != len(optimizer.param_groups):\n",
        "                raise ValueError(\"expected {} max_lr, got {}\".format(\n",
        "                    len(optimizer.param_groups), len(max_lr)))\n",
        "            self.max_lrs = list(max_lr)\n",
        "        else:\n",
        "            self.max_lrs = [max_lr] * len(optimizer.param_groups)\n",
        "\n",
        "        self.step_size = step_size\n",
        "\n",
        "        if mode not in ['triangular', 'triangular2', 'exp_range'] \\\n",
        "                and scale_fn is None:\n",
        "            raise ValueError('mode is invalid and scale_fn is None')\n",
        "\n",
        "        self.mode = mode\n",
        "        self.gamma = gamma\n",
        "\n",
        "        if scale_fn is None:\n",
        "            if self.mode == 'triangular':\n",
        "                self.scale_fn = self._triangular_scale_fn\n",
        "                self.scale_mode = 'cycle'\n",
        "            elif self.mode == 'triangular2':\n",
        "                self.scale_fn = self._triangular2_scale_fn\n",
        "                self.scale_mode = 'cycle'\n",
        "            elif self.mode == 'exp_range':\n",
        "                self.scale_fn = self._exp_range_scale_fn\n",
        "                self.scale_mode = 'iterations'\n",
        "        else:\n",
        "            self.scale_fn = scale_fn\n",
        "            self.scale_mode = scale_mode\n",
        "\n",
        "        self.batch_step(last_batch_iteration + 1)\n",
        "        self.last_batch_iteration = last_batch_iteration\n",
        "\n",
        "    def batch_step(self, batch_iteration=None):\n",
        "        if batch_iteration is None:\n",
        "            batch_iteration = self.last_batch_iteration + 1\n",
        "        self.last_batch_iteration = batch_iteration\n",
        "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
        "            param_group['lr'] = lr\n",
        "\n",
        "    def _triangular_scale_fn(self, x):\n",
        "        return 1.\n",
        "\n",
        "    def _triangular2_scale_fn(self, x):\n",
        "        return 1 / (2. ** (x - 1))\n",
        "\n",
        "    def _exp_range_scale_fn(self, x):\n",
        "        return self.gamma**(x)\n",
        "\n",
        "    def get_lr(self):\n",
        "        step_size = float(self.step_size)\n",
        "        cycle = np.floor(1 + self.last_batch_iteration / (2 * step_size))\n",
        "        x = np.abs(self.last_batch_iteration / step_size - 2 * cycle + 1)\n",
        "\n",
        "        lrs = []\n",
        "        param_lrs = zip(self.optimizer.param_groups, self.base_lrs, self.max_lrs)\n",
        "        for param_group, base_lr, max_lr in param_lrs:\n",
        "            base_height = (max_lr - base_lr) * np.maximum(0, (1 - x))\n",
        "            if self.scale_mode == 'cycle':\n",
        "                lr = base_lr + base_height * self.scale_fn(cycle)\n",
        "            else:\n",
        "                lr = base_lr + base_height * self.scale_fn(self.last_batch_iteration)\n",
        "            lrs.append(lr)\n",
        "        return lrs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Architecture**"
      ],
      "metadata": {
        "id": "F4DZVZsorjae"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFt4AZC9Ekrd"
      },
      "outputs": [],
      "source": [
        "class MyLSTM(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, hidden_dim_1, hidden_dim_2, hidden_dim_3, hidden_dim_4, lstm_layer=2):\n",
        "        super(MyLSTM, self).__init__()\n",
        "        \n",
        "        self.embedding = nn.Sequential(\n",
        "            nn.Conv1d(in_channels=input_dim, out_channels=hidden_dim_1,  kernel_size=3, padding=1, stride=1, bias=False),\n",
        "            nn.ReLU()\n",
        "        )  \n",
        "\n",
        "\n",
        "        self.lstm1 = nn.LSTM(input_size=hidden_dim_1, hidden_size=hidden_dim_2, num_layers=1, bidirectional=True)\n",
        "        self.conv_layers1 = nn.Sequential(\n",
        "            nn.Conv1d(in_channels=hidden_dim_2*2, out_channels=hidden_dim_1,  kernel_size=3, padding=1, stride=1, bias=False),\n",
        "            nn.ReLU()\n",
        "        )        \n",
        "\n",
        "\n",
        "        self.lstm2 = nn.LSTM(input_size=hidden_dim_2*2, hidden_size=hidden_dim_2, num_layers=1, bidirectional=True)\n",
        "        self.conv_layers2 = nn.Sequential(\n",
        "            nn.Conv1d(in_channels=hidden_dim_2*2, out_channels=hidden_dim_1,  kernel_size=5, padding=2, stride=1, bias=False),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(p=0.2)\n",
        "        \n",
        "        \n",
        "        self.fc1 = nn.Sequential(\n",
        "                                    nn.Linear(in_features=hidden_dim_1*4 + output_dim, out_features=hidden_dim_2),\n",
        "                                    nn.BatchNorm1d(hidden_dim_2),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.Dropout(0.2)\n",
        "                                )\n",
        "\n",
        "        self.fc2 = nn.Sequential(\n",
        "                            nn.Linear(in_features=hidden_dim_2, out_features=hidden_dim_3),\n",
        "                            nn.BatchNorm1d(hidden_dim_3),\n",
        "                            nn.ReLU(),\n",
        "                            nn.Dropout(0.2)\n",
        "                        )\n",
        "                \n",
        "        self.fc3 = nn.Sequential(\n",
        "                            nn.Linear(in_features=hidden_dim_3, out_features=hidden_dim_1),\n",
        "                            nn.BatchNorm1d(hidden_dim_1),\n",
        "                            nn.ReLU()\n",
        "                        ) \n",
        "\n",
        "        self.fc4 = nn.Linear(in_features=hidden_dim_1, out_features=output_dim)\n",
        "\n",
        "\n",
        "    def forward(self, x, y, z):\n",
        "        \"\"\"\n",
        "            x: Input data of past coils\n",
        "            y: Current info\n",
        "            z: Current added info\n",
        "        \"\"\"\n",
        "        # x: [batch_size, seq_len, embedding]\n",
        "        # Permute tensor to (batch_size, embedding_dim, seq_len)\n",
        "        x = x.permute(0, 2, 1)                      # (B, emb, seq_len)\n",
        "        x = self.embedding(x)\n",
        "        \n",
        "        x = x.permute(0, 2, 1)                      # (B, seq_len, emb)\n",
        "        out1, (h_n, c_n) = self.lstm1(x)            # (B, seq_len, hidden_dim_2*2)\n",
        "        \n",
        "        x = out1.permute(0, 2, 1)\n",
        "        x_1 = self.conv_layers1(x)\n",
        "        x_1 = nn.functional.max_pool1d(x_1, kernel_size=x_1.size()[2:])\n",
        "\n",
        "        x_2 = self.conv_layers2(x)\n",
        "        x_2 = nn.functional.max_pool1d(x_2, kernel_size=x_2.size()[2:])\n",
        "\n",
        "\n",
        "        out2, (h_n, c_n) = self.lstm2(out1)\n",
        "        \n",
        "        x = out2.permute(0, 2, 1)\n",
        "        y_1 = self.conv_layers1(x)\n",
        "        y_1 = nn.functional.max_pool1d(y_1, kernel_size=y_1.size()[2:])\n",
        "\n",
        "        y_2 = self.conv_layers2(x)\n",
        "        y_2 = nn.functional.max_pool1d(y_2, kernel_size=y_2.size()[2:])\n",
        "\n",
        "\n",
        "        x = torch.cat([x_1, x_2, y_1, y_2], dim=1).squeeze(2)\n",
        "        x = torch.cat([x, y], dim=1)\n",
        "        \n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.fc4(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpbwCiN7LaJ8"
      },
      "outputs": [],
      "source": [
        "num_epoch = 600"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Trainer**"
      ],
      "metadata": {
        "id": "oA4UlXL9roMa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ma7KoucJLMLX"
      },
      "outputs": [],
      "source": [
        "def train(model, optimizer, criterion, scheduler):\n",
        "  best_loss = 9999999\n",
        "  training_mae_loss, training_mse_loss = [], []\n",
        "  validation_mae_loss, validation_mse_loss = [], []\n",
        "  ep = 0\n",
        "    \n",
        "  if True in [x.endswith(\".pt\") for x in os.listdir(ckpt_dir)]:\n",
        "    try:\n",
        "      checkpoint = torch.load(f'{ckpt_dir}/{trial}_best.pt')\n",
        "      model.load_state_dict(checkpoint['model_state_dict'])\n",
        "      optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "      ep = checkpoint['epoch'] + 1\n",
        "      best_loss = checkpoint['best_loss']\n",
        "      print(f\"Resuming from previous episode: {ep}, best_loss: {best_loss}\")\n",
        "    except:\n",
        "      print(f\"Something wrong with {ckpt_dir}/{trial}_best.pt model. Please remove it!\")\n",
        "\n",
        "\n",
        "  for epoch in range(ep, num_epoch):\n",
        "    # Here starts the train loop.\n",
        "    model.train()\n",
        "\n",
        "    N_total = 0.\n",
        "    MAE_total = 0.\n",
        "    MSE_total = 0.\n",
        "    R_SQUARE_total = 0.\n",
        "\n",
        "    for batch_idx, data in tqdm(enumerate(train_dataloader)):\n",
        "      input = data['input'].cuda()\n",
        "      prev = data['prev'].cuda()\n",
        "      additional = data['additional'].cuda()\n",
        "      \n",
        "      y_pred = model(prev, input, additional)\n",
        "\n",
        "      loss = criterion(y_pred, input)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "\n",
        "      optimizer.step()\n",
        "      scheduler.batch_step()\n",
        "\n",
        "      # Update the metrics for this batch\n",
        "      batch_mae = torch.mean(torch.abs(y_pred - input), dim=0)\n",
        "      batch_mse = torch.mean((y_pred - input)**2, dim=0)\n",
        "\n",
        "      y_mean = torch.mean(input, dim=0, keepdim=True)\n",
        "      y_diff = input - y_pred\n",
        "\n",
        "      batch_r_squared = 1 - torch.sum(y_diff**2, dim=0) / torch.sum((input - y_mean)**2, dim=0)\n",
        "\n",
        "      # Accumulate the metrics for this epoch\n",
        "      N_total += len(data)\n",
        "      MAE_total += torch.sum(batch_mae).item()\n",
        "      MSE_total += torch.sum(batch_mse).item()\n",
        "      \n",
        "\n",
        "    # L_total /= N_total\n",
        "    MAE_total /= N_total\n",
        "    MSE_total /= N_total\n",
        "\n",
        "    training_mae_loss.append(MAE_total)\n",
        "    training_mse_loss.append(MSE_total)\n",
        "\n",
        "    print(f'Training result {epoch}/{num_epoch} || MSE loss: {MSE_total:.5f} || MAE loss: {MAE_total:.5f}')\n",
        "    \n",
        "    # Here starts the validation loop.\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      N_total_ = 0.\n",
        "      MAE_total_ = 0.\n",
        "      MSE_total_ = 0.\n",
        "      R_SQUARE_total_ = 0.\n",
        "\n",
        "      for batch_idx, data in tqdm(enumerate(val_dataloader)):\n",
        "        input = data['input'].cuda()\n",
        "        prev = data['prev'].cuda()\n",
        "        additional = data['additional'].cuda()\n",
        "        \n",
        "        y_pred = model(prev, input, additional)\n",
        "        \n",
        "        loss = criterion(y_pred, input)\n",
        "\n",
        "        # Update the metrics for this batch\n",
        "        batch_mae = torch.mean(torch.abs(y_pred - input), dim=0)\n",
        "        batch_mse = torch.mean((y_pred - input)**2, dim=0)\n",
        "\n",
        "        y_mean = torch.mean(input, dim=0, keepdim=True)\n",
        "        y_diff = input - y_pred\n",
        "        \n",
        "        batch_r_squared = 1 - torch.sum(y_diff**2, dim=0) / torch.sum((input - y_mean)**2, dim=0)\n",
        "\n",
        "        # Accumulate the metrics for this epoch\n",
        "        N_total_ += len(data)\n",
        "        MAE_total_ += torch.sum(batch_mae).item()\n",
        "        MSE_total_ += torch.sum(batch_mse).item()\n",
        "\n",
        "      #L_total_ /= N_total_\n",
        "      MAE_total_ /= N_total_\n",
        "      MSE_total_ /= N_total_\n",
        "\n",
        "      validation_mae_loss.append(MAE_total_)\n",
        "      validation_mse_loss.append(MSE_total_)\n",
        "\n",
        "      print(f'\\tValidation result {epoch}/{num_epoch} || MSE loss: {MSE_total_:.5f} || MAE loss: {MAE_total_:.5f}')\n",
        "      \n",
        "      # Whenever `test_accuracy` is greater than `best_accuracy`, save network weights with the filename 'best.pt' in the directory specified by `ckpt_dir`.\n",
        "      if MAE_total_ < best_loss:\n",
        "        best_loss = MAE_total_\n",
        "        torch.save(\n",
        "            {\n",
        "              'epoch': epoch,\n",
        "              'model_state_dict': model.state_dict(),\n",
        "              'optimizer_state_dict': optimizer.state_dict(),\n",
        "              'best_loss': best_loss\n",
        "            },\n",
        "            f'{ckpt_dir}/{trial}_best.pt')\n",
        "  \n",
        "  torch.save(\n",
        "              {\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'best_loss': best_loss\n",
        "              },\n",
        "              f'{ckpt_dir}/{trial}_all.pt')\n",
        "  return training_mae_loss, training_mse_loss, validation_mae_loss, validation_mse_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0geso67GMr1"
      },
      "outputs": [],
      "source": [
        "input_dim = train_dataset[0][\"prev\"].size()[1]\n",
        "output_dim = train_dataset[0][\"input\"].size()[0]\n",
        "hidden_dim_1 = 128\n",
        "hidden_dim_2 = 64\n",
        "hidden_dim_3 = 256\n",
        "hidden_dim_4 = train_dataset[0][\"additional\"].size()[0]\n",
        "lstm_layer = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-L4ZNJ3dGO0u"
      },
      "outputs": [],
      "source": [
        "model = MyLSTM(input_dim = input_dim,\n",
        "               output_dim = output_dim,\n",
        "               hidden_dim_1 = hidden_dim_1,\n",
        "               hidden_dim_2 = hidden_dim_2,\n",
        "               hidden_dim_3 = hidden_dim_3,\n",
        "               hidden_dim_4 = hidden_dim_4,\n",
        "               lstm_layer = lstm_layer).cuda()\n",
        "\n",
        "# criterion = torch.nn.MSELoss()\n",
        "criterion = torch.nn.L1Loss()\n",
        "\n",
        "base_lr, max_lr = 0.001, 0.003\n",
        "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=max_lr)\n",
        "\n",
        "step_size = 300\n",
        "\n",
        "scheduler = CyclicLR(optimizer, base_lr=base_lr, max_lr=max_lr,\n",
        "               step_size=step_size, mode='exp_range',\n",
        "               gamma=0.99994)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xIu43erLMHtq"
      },
      "outputs": [],
      "source": [
        "train_mae_loss, train_mse_loss, val_mae_loss, val_mse_loss = train(model=model, optimizer=optimizer, criterion=criterion, scheduler=scheduler)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Plot Loss**"
      ],
      "metadata": {
        "id": "UZ9s_Zi6ru38"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STDFbkGdSxjq"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scatter(y= train_mae_loss,\n",
        "                    mode='lines',\n",
        "                    name='MAE Train Loss'))\n",
        "fig.add_trace(go.Scatter(y=val_mae_loss,\n",
        "                    mode='lines',\n",
        "                    name='MAE Val Loss'))\n",
        "fig.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scatter(y= train_mse_loss,\n",
        "                    mode='lines',\n",
        "                    name='MSE Train Loss'))\n",
        "fig.add_trace(go.Scatter(y=val_mse_loss,\n",
        "                    mode='lines',\n",
        "                    name='MSE Val Loss'))\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGHAUyokvcjB"
      },
      "source": [
        "### **Testing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4reZmwo99H5F"
      },
      "outputs": [],
      "source": [
        "criterion_1 = torch.nn.L1Loss()\n",
        "criterion_2 = torch.nn.MSELoss()\n",
        "criterion_3 = torch.nn.CosineSimilarity(dim=1, eps=1e-6)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Load Desired Model**"
      ],
      "metadata": {
        "id": "ecr_-HdcsLd1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZzjPRYpy4Syi"
      },
      "outputs": [],
      "source": [
        "new_trial = 0\n",
        "model_path_ = Path(f'{root}/logs_competition/ckpt_{new_trial}/{new_trial}_all.pt')\n",
        "\n",
        "ckpt_ = torch.load(model_path_)\n",
        "print(f\"Best MAE LOSS: {ckpt_['best_loss']} after training for {ckpt_['epoch']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hh_LzdrTXa-D"
      },
      "outputs": [],
      "source": [
        "model_ = MyLSTM(input_dim = input_dim,\n",
        "               output_dim = output_dim,\n",
        "               hidden_dim_1 = hidden_dim_1,\n",
        "               hidden_dim_2 = hidden_dim_2,\n",
        "               hidden_dim_3 = hidden_dim_3,\n",
        "               hidden_dim_4 = hidden_dim_4,\n",
        "               lstm_layer = lstm_layer).cuda()\n",
        "\n",
        "model_.load_state_dict(ckpt_[\"model_state_dict\"])\n",
        "model_.eval()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_on_idx = [4, 5]      # Select the class types you want to test on\n",
        "df_test = filter_data(test, test_on_idx)"
      ],
      "metadata": {
        "id": "NERYNlNQr1TX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Transform Test Data**"
      ],
      "metadata": {
        "id": "tysPybrlr8cn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = scaler.transform(df_test[columns])\n",
        "test_data = pd.DataFrame(data=test_data, index=df_test.index.tolist(), columns=columns)\n",
        "test_data = pd.concat([test_data, df_test[['type']]], axis=1)\n",
        "test_data.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "rLg5F_xirrsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = MyDataset(data=test_data, features=features, added_features=added_features, look_forward=[0], look_backward=[2], duplicate=0)"
      ],
      "metadata": {
        "id": "sqr-cASdsJP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataloader = DataLoader(dataset=test_dataset, batch_size=1, shuffle=False)"
      ],
      "metadata": {
        "id": "6iXjdM8ssgfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-T3K3Qccrml"
      },
      "outputs": [],
      "source": [
        "mae_diff = []\n",
        "mse_diff = []\n",
        "cosine_similarity = []\n",
        "mae_original = []\n",
        "\n",
        "for data in tqdm(test_dataloader):\n",
        "        input = data['input'].cuda()\n",
        "        prev = data['prev'].cuda()\n",
        "        additional = data['additional'].cuda()\n",
        "      \n",
        "        xout = model_(prev, input, additional)\n",
        "\n",
        "        in_val = input.squeeze(0).cpu().detach().numpy()\n",
        "        in_val *= train_inv_transform.loc[\"stdv\", features]   \n",
        "        in_val += train_inv_transform.loc[\"mean\", features]\n",
        "\n",
        "        out_val = xout.squeeze(0).cpu().detach().numpy()\n",
        "        out_val *= train_inv_transform.loc[\"stdv\", features]   \n",
        "        out_val += train_inv_transform.loc[\"mean\", features]\n",
        "\n",
        "        mae = criterion_1(xout, input)\n",
        "        mse = criterion_2(xout, input)\n",
        "        cos = criterion_3(xout, input)\n",
        "        mae_orig = criterion_1(torch.FloatTensor(out_val), torch.FloatTensor(in_val))\n",
        "\n",
        "        mae_diff.append(float(mae.cpu().detach()))\n",
        "        mse_diff.append(float(mse.cpu().detach()))\n",
        "        cosine_similarity.append(float(cos.cpu().detach()))\n",
        "        mae_original.append(float(mae_orig.cpu().detach()))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Plot Reconstruction Error**"
      ],
      "metadata": {
        "id": "nebHshKQsCem"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EcTMA42AYYie"
      },
      "outputs": [],
      "source": [
        "fig = px.scatter(x=df_test.index.tolist(), y=mae_diff, color=df_test[\"type\"])\n",
        "fig.update_layout(title=str(test_on_idx))\n",
        "fig.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}